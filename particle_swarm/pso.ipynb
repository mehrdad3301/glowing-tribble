{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a50291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be954abb",
   "metadata": {},
   "source": [
    "we chose to use breast cancer dataset in our expriments because of its various features. \n",
    "the dataset contains `30` features. if we were to brute force the search on these many feature, we would have to go through 2 to the power of 30 different combinations and that will take forever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c545ad43",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "x, y = data.data, data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d12de",
   "metadata": {},
   "source": [
    "we first use `KNN` with only `1` neighbors to see how well the model does without any optimized feature selection algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b84935ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5447c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9202127659574468\n"
     ]
    }
   ],
   "source": [
    "KNN.fit(x_train, y_train) \n",
    "print(f'accuracy: {KNN.score(x_test, y_test)}'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e51eb",
   "metadata": {},
   "source": [
    "in the next cell we will use `PSO` to select a subset of features. as you can see the model has a performance boost of about `6%` !. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0203e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best result at iteration 0: 0.9632253711201079\n",
      "best result at iteration 1: 0.9659244264507422\n",
      "best result at iteration 2: 0.9632253711201079\n",
      "best result at iteration 3: 0.9684885290148447\n",
      "best result at iteration 4: 0.9684885290148447\n",
      "best result at iteration 5: 0.9684885290148447\n",
      "best result at iteration 6: 0.9684885290148447\n",
      "best result at iteration 7: 0.9684885290148447\n",
      "best result at iteration 8: 0.9684885290148447\n",
      "best result at iteration 9: 0.9684885290148447\n",
      "best result on the test set -> 0.9787234042553191\n"
     ]
    }
   ],
   "source": [
    "model = modelWrapper(x, y) \n",
    "p = PSO(30, x.shape[1], model) \n",
    "features = p.optimize(num_iter=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f44ac6",
   "metadata": {},
   "source": [
    "next we will test to see how the new `HPSOLS` algorithm does on our dataset. \n",
    "the results are disappointing. we expected the model to do much better than `PSO`. it didn't, \n",
    "and after hour of debugging i still don't know why. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "729706ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best result at iteration 0: 0.9475708502024291\n",
      "best result at iteration 1: 0.9579622132253711\n",
      "best result at iteration 2: 0.9579622132253711\n",
      "best result at iteration 3: 0.9422402159244264\n",
      "best result at iteration 4: 0.9449392712550606\n",
      "best result at iteration 5: 0.9422402159244264\n",
      "best result at iteration 6: 0.9449392712550606\n",
      "best result at iteration 7: 0.9449392712550606\n",
      "best result at iteration 8: 0.9422402159244264\n",
      "best result at iteration 9: 0.9475708502024291\n",
      "best result on the test set -> 0.9787234042553191\n"
     ]
    }
   ],
   "source": [
    "p = HPSOLS(30, x.shape[1], model)\n",
    "features = p.optimize(num_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1d0824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class modelWrapper():\n",
    "    \"\"\"\n",
    "    modelWrapper is used to handle basic operations \n",
    "    related to the dataset. it uses a model to make evaluations\n",
    "    which will be used by PSO algorithm to determine the best particle.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, model=None):\n",
    "        self.model = model or KNeighborsClassifier(n_neighbors=5)\n",
    "        self.x_train, self.x_test, \\\n",
    "        self.y_train, self.y_test = train_test_split(self.normalize(x), y, test_size=0.33)\n",
    "        self.init_corr()\n",
    "\n",
    "    def evaluate(self, features):\n",
    "        \"\"\"\n",
    "        evaluate is uses the given features and \n",
    "        returns mean accuracy using k-fold cross validation\n",
    "        \"\"\"\n",
    "        if np.all(features == 0) : \n",
    "            return 0 \n",
    "\n",
    "        x_train = self.x_train[:, np.where(features)[0]]\n",
    "        return cross_val_score(self.model,\n",
    "                                x_train, \n",
    "                                self.y_train, \n",
    "                                cv=10,\n",
    "                                n_jobs=-1, \n",
    "                                error_score=0).mean()\n",
    "\n",
    "    def test(self, features) : \n",
    "        \"\"\"\n",
    "        test uses the given features to fit and evaluate the test set \n",
    "        \"\"\"\n",
    "        x_test = self.x_test[:, np.where(features)[0]]\n",
    "        x_train = self.x_train[:, np.where(features)[0]]\n",
    "        self.model.fit(x_train, self.y_train) \n",
    "        return self.model.score(x_test, self.y_test) \n",
    "\n",
    "    def init_corr(self):\n",
    "        \"\"\"\n",
    "        init_corr first creates the correlation matrix, \n",
    "        it then initializes the correlations specified by the paper \n",
    "        \"\"\"\n",
    "        _, n = self.x_train.shape\n",
    "        c = np.zeros(shape=(n, n))\n",
    "            # TODO this can be replaced by a library function\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                x, y = self.x_train[:, i], self.x_train[:, j]\n",
    "                c[i][j] = np.sum((x - x.mean()) *\n",
    "                                (y - y.mean()) / np.std(x) / np.std(y))\n",
    "        self.corr = np.sum(c, axis=1) / (n - 1)\n",
    "    \n",
    "    def normalize(self, x) : \n",
    "        l , u = -1, 1\n",
    "        x = l + (u - l) * (x - np.min(x, axis=0)) / (np.max(x, axis=0) - np.min(x, axis=0))\n",
    "        return x \n",
    "\n",
    "class Particle:\n",
    "    \"\"\"\n",
    "    particle class represents each particle in particle swarm\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension):\n",
    "        self.position = np.random.randint(0, 2, size=(dimension))\n",
    "        self.velocity = np.zeros(dimension)  \n",
    "        self.memory = self.position\n",
    "\n",
    "\n",
    "class PSO:\n",
    "    \"\"\"\n",
    "    PSO is responsible for implementing basic particle swarm algorithms. \n",
    "    it is then subclassed by HPSOLS. \n",
    "    the code is very readable with minimal need for explanation.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_particles=50,\n",
    "                 dimension=30,\n",
    "                 model=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.dimension = dimension\n",
    "        self.particles = [Particle(dimension) for _ in range(num_particles)]\n",
    "        self.update_global()\n",
    "\n",
    "    def optimize(self, num_iter=10, w=0.8, c1=2, c2=2):\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            self.update_particles(w, c1, c2)\n",
    "            print(f'best result at iteration {i}: {self.model.evaluate(self.best.position)}')\n",
    "        print(f'best result on the test set -> {model.test(self.best.position)}')\n",
    "        return self.best.position\n",
    "\n",
    "    def update_particles(self, w, c1, c2):\n",
    "        for p in self.particles:\n",
    "            self.update_position(p, w, c1, c2)\n",
    "            self.update_memory(p)\n",
    "        self.update_global()\n",
    "\n",
    "    def update_position(self, p, w, c1, c2):\n",
    "        r1, r2 = np.random.rand(2)\n",
    "        p.velocity = w * p.velocity + r1 * c1 * (p.memory - p.position) * \\\n",
    "            np.linalg.norm(p.velocity, 2) + r2 * c2 * (self.best.position - p.position)\n",
    "        p.velocity = np.clip(p.velocity, -4, 4)\n",
    "        p.position = (np.random.rand()\n",
    "                      < expit(p.velocity)).astype(int)\n",
    "\n",
    "    def update_memory(self, p):\n",
    "        if self.model.evaluate(p.position) > self.model.evaluate(p.memory):\n",
    "            p.memory = p.position\n",
    "\n",
    "    def update_global(self):\n",
    "        scores = [self.model.evaluate(p.position) for p in self.particles]\n",
    "        self.best = self.particles[np.argmax(scores)]\n",
    "\n",
    "\n",
    "class HPSOLS(PSO):\n",
    "    \"\"\"\n",
    "    HPSOLS stands for `hybrid particle swarm optimization with local search`. \n",
    "    it a very fancy name for a search algorithm. HPSOLS inherits PSO and \n",
    "    implements the added functionalities specified by the paper. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_particles=50,\n",
    "                 dimension=30,\n",
    "                 model=None,\n",
    "                 eps=None):\n",
    "\n",
    "        super(HPSOLS, self).__init__(num_particles, dimension, model)\n",
    "        self.init_num_features(eps)\n",
    "        self.init_feature_set()\n",
    "\n",
    "    def optimize(self, num_iter=10, alpha=0.65, w=0.8, c1=2, c2=2):\n",
    "        for i in range(num_iter):\n",
    "            self.update_particles(w, c1, c2)\n",
    "            self.move_particles(alpha)\n",
    "            print(f'best result at iteration {i}: {self.model.evaluate(self.best.position)}')\n",
    "        print(f'best result on the test set -> {model.test(self.best.position)}')\n",
    "        return self.best.position\n",
    "\n",
    "    def move_particles(self, alpha):\n",
    "        for p in self.particles:\n",
    "            self.move_particle(p, alpha)\n",
    "\n",
    "    def move_particle(self, p, alpha) :\n",
    "        \"\"\"\n",
    "        move_particle performs the local search specified by the algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        nd, ns = int(np.ceil(alpha * self.num_features)),\\\n",
    "                 int(np.floor((1 - alpha) * self.num_features))\n",
    "\n",
    "        f = np.where(p.position) \n",
    "        fd = list(np.intersect1d(self.dissimilars, f))\n",
    "        fs = list(np.intersect1d(self.similars, f))\n",
    "\n",
    "        fd = self.add_features(nd, fd, self.dissimilars)\n",
    "        fd = self.delete_features(nd, fd, self.dissimilars)\n",
    "\n",
    "        fs = self.add_features(ns, fs, self.similars[::-1])\n",
    "        fs = self.delete_features(ns, fs, self.similars[::-1])\n",
    "\n",
    "        p.position = np.zeros(self.dimension)\n",
    "        p.position[fs + fd] = 1 \n",
    "\n",
    "    def add_features(self, n, f, s):\n",
    "        i = len(s) - 1\n",
    "        while len(f) < n : \n",
    "            while s[i] in f : \n",
    "                i -= 1 \n",
    "            f.append(s[i])\n",
    "        return f \n",
    "\n",
    "    def delete_features(self, n, f, s):\n",
    "        i = 0\n",
    "        while len(f) > n : \n",
    "            while s[i] not in f : \n",
    "                i += 1 \n",
    "            f.remove(s[i]) \n",
    "        return f \n",
    "\n",
    "    def init_feature_set(self):\n",
    "        c = np.argsort(self.model.corr)\n",
    "        self.similars = c[len(c) // 2:]\n",
    "        self.dissimilars = c[:len(c) // 2][::-1]\n",
    "\n",
    "    def init_num_features(self, eps):\n",
    "\n",
    "        eps = self.init_eps(eps)\n",
    "        f = self.dimension\n",
    "        k = int(eps * f)\n",
    "        sf = np.arange(3, f)\n",
    "        denom = np.cumsum(sf[::-1])\n",
    "        prob = (f - sf) / denom[::-1]\n",
    "        prob = prob[:k] / np.sum(prob[:k])\n",
    "        self.num_features = np.random.choice(sf[:k], p=prob)\n",
    "\n",
    "    def init_eps(self, eps):\n",
    "        if not eps:\n",
    "            eps = 0\n",
    "            while eps < 0.15 or eps > 0.7:\n",
    "                eps = np.random.rand(1)\n",
    "        return eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01a43ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
